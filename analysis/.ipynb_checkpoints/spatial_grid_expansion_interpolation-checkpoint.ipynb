{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import random\n",
    "import libpysal\n",
    "import pysal\n",
    "import pysal.lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from URL (Doesnt work for private repos)\n",
    "#url=\"https://github.com/konstantinklemmer/spacegan/raw/master/data/synth_data.csv\"\n",
    "#s=requests.get(url).content\n",
    "#data=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "#Read from local file\n",
    "data_ex1 = pd.read_csv(\"C:/Users/Konstantin Klemmer/Documents/GitHub/spacegan/data/synth_data_ex1.csv\")\n",
    "data_ex2 = pd.read_csv(\"C:/Users/Konstantin Klemmer/Documents/GitHub/spacegan/data/synth_data_ex2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Grid Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a regular point grid `data` (with coordinates `longitude` and `latitude`, input variable `z` and output variable `y`) and a factor `fact` by which the grid is to be expanded (e.g., `fact=2` doubles the grid cells (roughly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_expand(data,fact):\n",
    "    \n",
    "    #Create lon and lat groups (for the case that the test data splits our regular box in two separate training sets)\n",
    "    lon_unique = data.longitude.unique() #Get unique lon/lat values\n",
    "    inp = np.insert(np.diff(data.longitude.unique()),0,np.min(np.diff(data.longitude.unique()))) #Get differences at each step\n",
    "    res = []\n",
    "    k = 1\n",
    "    for i in inp: #Loop over differences and assign new group if a gap is detected\n",
    "        if i != np.min(inp):\n",
    "            k = k+1\n",
    "        else:\n",
    "            k = k\n",
    "        res += [k]\n",
    "\n",
    "    d = {'longitude': lon_unique, 'group_lon': res} #Bind into a dataframe\n",
    "    lon_group = pd.DataFrame(data=d)\n",
    "    #Repeat for latitude\n",
    "    lat_unique = data.latitude.unique()\n",
    "    inp = np.insert(np.diff(data.latitude.unique()),0,np.min(np.diff(data.latitude.unique())))\n",
    "    res = []\n",
    "    k = 1\n",
    "    for i in inp:\n",
    "        if i != np.min(inp):\n",
    "            k = k+1\n",
    "        else:\n",
    "            k = k\n",
    "        res += [k]\n",
    "    d = {'latitude': lat_unique, 'group_lat': res}\n",
    "    lat_group = pd.DataFrame(data=d)\n",
    "    #Merge with input data\n",
    "    data = pd.merge(lon_group,data, how=\"right\", on=[\"longitude\"])\n",
    "    data = pd.merge(lat_group,data, how=\"right\", on=[\"latitude\"])\n",
    "    #Create dataset for storing the final results\n",
    "    final_data = pd.DataFrame()\n",
    "    \n",
    "    for o in data.group_lat.unique():\n",
    "        #Select group\n",
    "        temp1_data = data[data[\"group_lat\"]==o]\n",
    "        for p in temp1_data.group_lon.unique():\n",
    "            #Select group\n",
    "            temp_data = temp1_data[temp1_data[\"group_lon\"]==p]\n",
    "            #Create new grid:\n",
    "            #Copy column names from existing grid\n",
    "            new_points = pd.DataFrame(columns=temp_data[[\"longitude\",\"latitude\"]].columns)\n",
    "            #Expand longitude by factor `fact`\n",
    "            dst = (temp_data[[\"longitude\"]].max() - temp_data[[\"longitude\"]].min()) / (len(temp_data.longitude.unique())-1)\n",
    "            exp = float(dst/fact)\n",
    "            x = []\n",
    "            d = float(temp_data[[\"longitude\"]].min())\n",
    "            while d <= float(temp_data[[\"longitude\"]].max()):\n",
    "                x.append(d)    \n",
    "                d = d + exp\n",
    "            #Expand latitude by factor `fact`\n",
    "            dst = (temp_data[[\"latitude\"]].max() - temp_data[[\"latitude\"]].min()) / (len(temp_data.latitude.unique())-1)\n",
    "            exp = float(dst/fact)\n",
    "            y = []\n",
    "            d = float(temp_data[[\"latitude\"]].min())\n",
    "            while d <= float(temp_data[[\"latitude\"]].max()):\n",
    "                y.append(d)    \n",
    "                d = d + exp\n",
    "            #Bind new lat and lon values\n",
    "            lon = x * len(y)\n",
    "            lat = sorted(y*len(x))\n",
    "            #Create new points dataframe\n",
    "            new_points = pd.DataFrame({\"longitude\":lon,\"latitude\":lat})\n",
    "            #Merge with existing dataframe\n",
    "            new_data = pd.merge(new_points,data, how=\"left\", on=[\"longitude\",\"latitude\"])\n",
    "            #Create column to indicate whether observation is original or synthetic \n",
    "            new_data[\"synth\"] = 0\n",
    "            new_data.loc[new_data.id.isnull(), \"synth\"] = 1\n",
    "\n",
    "            #Spatial operations:\n",
    "            #Compute distance matrix of new grid\n",
    "            dist = pysal.lib.cg.distance_matrix(np.array(new_data[[\"longitude\",\"latitude\"]]))\n",
    "            #Extract and flatten 10 nearest distances\n",
    "            k=10\n",
    "            u_dist = np.unique(dist)\n",
    "            k_min_dist = np.sort(u_dist.flatten())[:k]\n",
    "            #Create KD tree\n",
    "            kd = pysal.lib.cg.kdtree.KDTree(np.array(new_data[[\"longitude\",\"latitude\"]]))\n",
    "            #Define neighbourhood (here Queen structure)\n",
    "            w = pysal.lib.weights.distance.DistanceBand(kd, threshold=k_min_dist[2],binary=True,p=2) #Queen\n",
    "\n",
    "            #Interpolate missing grid values\n",
    "            #OPTION 1:\n",
    "            #Interpolate new grid data based on mean of neighborhood\n",
    "            for k in new_data.index:\n",
    "                fill = new_data.iloc[k]\n",
    "                temp_id = []\n",
    "                if fill[\"synth\"] == 1:\n",
    "                    temp_id = np.unique(np.concatenate([temp_id,w.neighbors[k]]).ravel().astype(np.int32))\n",
    "                    neighbors = new_data.iloc[temp_id]\n",
    "                    fill[\"z\"] = np.mean(neighbors[\"z\"])\n",
    "                    fill[\"y\"] = np.mean(neighbors[\"y\"]) #Delete this line if OPTION 2 is preferred\n",
    "                    new_data.iloc[k] = fill\n",
    "                else:\n",
    "                    continue \n",
    "            #Bind to existing data\n",
    "            final_data = pd.concat([final_data, new_data])\n",
    "            \n",
    "#     #OPTION 2:\n",
    "#     #Interpolate Y using a linear model (works very poorly)\n",
    "\n",
    "#     #from sklearn.linear_model import LinearRegression\n",
    "#     #train = new_data[new_data[\"synth\"]==0]\n",
    "#     #test = new_data[new_data[\"synth\"]==1]\n",
    "#     #x_train = train[[\"longitude\",\"latitude\",\"z\"]]\n",
    "#     #y_train = train[[\"y\"]]\n",
    "#     #x_test = test[[\"longitude\",\"latitude\",\"z\"]]\n",
    "#     #reg = LinearRegression().fit(x_train,y_train)\n",
    "#     #y_pred = reg.predict(x_test)\n",
    "#     #new_data[\"y\"][new_data[\"synth\"]==1] = y_pred\n",
    "    \n",
    "    new_data = final_data\n",
    "    #Re-assign IDs\n",
    "    final_data[[\"id\"]] = np.asarray(list(range(0,len(final_data[\"id\"])))).reshape(-1,1)\n",
    "    #Delete helper columns\n",
    "    final_data = final_data.drop(columns=[\"group_lat\",\"group_lon\"])\n",
    "    #Return new dataframe\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with both synthetic grid datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_ex1 = grid_expand(data_ex1,2)\n",
    "new_data_ex2 = grid_expand(data_ex2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the newly expanded dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_ex1.to_csv(\"grid_expanded_ex1.csv\")\n",
    "new_data_ex2.to_csv(\"grid_expanded_ex2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.scatter(data_ex1[\"longitude\"],data_ex1[\"latitude\"],c=data_ex1[\"y\"])\n",
    "#plt.scatter(new_data_ex1[\"longitude\"],new_data_ex1[\"latitude\"],c=new_data_ex1[\"y\"])\n",
    "plt.scatter(new_data_ex1[\"longitude\"],new_data_ex1[\"latitude\"],c=\"red\")\n",
    "plt.scatter(data_ex1[\"longitude\"],data_ex1[\"latitude\"],c=\"blue\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
